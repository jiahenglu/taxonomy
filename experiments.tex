\section{Experimental analysis}

In this section, we evaluate the experimental results of
the proposed exact-join and approximate-join algorithms.

 We will compare
their effectiveness on the following datasets from different
application scenarios in Java $1.6.0$ and run on a
Windows XP with dual-core Intel Xeon CPU 4.0GHz, 2GB RAM, and a 320GB hard disk.


\subsection{Datasets}

One possible dataset is about the model of CPU:
%http://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/

%http://arthropods.eugenes.org/lucegene_arthropod/search

%http://arthropods.eugenes.org/lucegene_arthropod/resultxsl.jsp




To evaluate the impact of taxonomy information on classification performance, we generated taxonomies over each dataset.
While aggregation trees on continuous attributes have been generated by applying discretization procedures, aggregation trees
on nominal attributes are analyst-provided. Note that, in real-life contexts, aggregation trees may be semi-automatically derived, for instance, from lexical or geographical databases (e.g., the WordNet Lexical Database [18]). A more detailed description of the
applied multiple-taxonomies follows.

As a subject matter taxonomy for our expert matching
in this medicine domain, we used the Medical Subject Head-
ings (MeSH)\footnote{https://www.nlm.nih.gov/mesh} that is a biomedical controlled vocabulary
consisting of 26k+ biomedical terms arranged in a taxonomic
structure introduced by National Library of Medicine
(NLM). The benefits of using MeSH terms have been increasingly
highlighted in various applications for biomedical
concept and knowledge extraction from text [11].


We use three datasets: Consensus data (\textbf{Consensus}),
Academic data  (\textbf{Academy}), and Life science data
(\textbf{Life}). These datasets differ from each other in terms of rule-number, rule-complexity, data-size and string-length. Our goal in choosing these diverse sources is to understand the usefulness of algorithms in different real world environments.

\smallskip
\noindent \textbf{{Consensus data}}: Extraction was done by Barry Becker from the 1994 Census database. native-country: Include 32561 instances, we can perform the similarity joins on multiple column, including countries, occupation, marital-status, education and workclass. The feature of this dataset is to perform the similarity join against multiple columns.

\noindent \textbf{{Academic data}}: We collected 10,000 affiliations of authors for computer science.
We build a taxonomy tree which include the country and city and the university names.


\noindent \textbf{{Life science data}}: Designing data integration solutions in the context of the Life Sciences
must take into account the specific properties of the domain [3,
4]. These are related to (i) the way biological data is produced
and stored within biological databases (BDB), (ii) the fact that
biology is a science in constant evolution, and (iii) the fact
that BDB users are from various communities.
Regarding the first point, primary BDB store data that is
close to the raw experimental result (e.g., DNA sequences),
while secondary BDB manage information obtained after
various steps of analysis and careful curation, grouped around
increasingly complex entities (e.g., genes, diseases, pathways
etc.). While (discrete, string-type) sequences were the main
kind of raw data to integrate 20 years ago, today various ¨C
omics data set (transcriptomics, proteomics,etc.), often
consisting of quantitative measurements, are of equal
importance. Secondary BDBs heavily import data from other
BDB and are mostly maintained by human curators; they may
be complementary but also could be redundant and divergent
(especially when experts disagree). Because humans offer
unrivaled precision, manual curation still predominates,
despite its high cost and obvious problems of scalability.
The number of BDB is increasing steeply; currently more
than 1,200 BDBs are publicly available. This led to the call
for DI systems with extreme scalability in the number of
sources. However, in reality the usage of these databases is
Zipf-distributed [2], and there are only a few projects that
work with more than, say, 20 different data sources.
We obtained one million gene/protein records
from the Expasy website ({\footnotesize http://www.expasy.ch/sprot}).
Each record contains an identifier (ID) and its name.
%For example, the two records, \textit{(O00203, Adapter-related protein complex 3
%beta-1 subunit)} and \textit{(O00203, AP-3 complex subunit
%beta-1)}, refer to the same gene as they have the same ID.
In this dataset, each ID has $5\sim22$ synonyms. We generated 10,000 synonym rules describing gene/protein
equivalent expressions.


\begin{table}[t]
\centering
\begin{tabular}{|@{\hspace{1mm}}c@{\hspace{1mm}}|@{\hspace{1mm}}c@{\hspace{1mm}}|@{\hspace{1mm}}c@{\hspace{1mm}}|@{\hspace{1mm}}c@{\hspace{1mm}}|}
%{|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|}
%|c|m{6cm}|
\hline
 \textbf{Dataset} & \textbf{\# of strings} &  \textbf{string length} & \textbf{Taxonomy size} \\
  \hline \hline

  Consensus & 10,000 & 5 & 2,000 \\

   Academy & 10,000 & 5 &  2,000 \\

   Life & 10,000  & 5 & 2,000  \\

  \hline
\end{tabular}
\caption{Time and  I/O cost and performance}
\label{tab:data}
\end{table}


\begin{figure}
  \small
  \centering
  \includegraphics[width=\linewidth]{figures/Characteristics_Datasets}
   \vspace{-6mm}
  \caption{Characteristics of Datasets.}
  \label{tab:data_characteristics}
\end{figure}


Figure~\ref{tab:data_characteristics} gives the characteristics of the
three datasets.

\subsection{Effectiveness of taxonomy for string joins}

The first experiment is to demonstrate the effectiveness of
taxonomy for string joins. We compared our
two measures: Jaccard similarity, full expansion
  (\textbf{Full}), and  our similarity with taxonomy.

For each of the three datasets, we performed the experiments by conducting a similarity join between the
query table $T_Q$ and the target table $T_T$ as follows: (1) $T_Q$ consists of 100 manually selected
full names, and (2) $T_T$ has 500 records where 100 of them
are the correct abbreviations of the corresponding records in $T_Q$
(i.e., the ground truth), and the other 400 random records are selected from string collections. This is to
ensure that there is only one correct matching record in $T_T$ for each
 record in $T_Q$.

Quality of Measures.

In Figure \ref{fig:quality}, we report the quality of the measures by testing the \textit{Precision} (short for ``P''),  \textit{Recall} (``R''), and \textit{F-measure$=\frac{2\times P \times R}{P+R}$} (``F'') on three datasets. We observe that:

\noindent$\bullet$ The similarity measures using synonyms (including \textit{JaccT}, \textit{Full} and \textit{SE}) obtain higher scores than Jaccard which does not consider synonym pairs. The reason is that without using synonyms, Jaccard has no chance to improve the similarity.

%\noindent$\bullet$  \textit{Full} achieves a comparable performance with \textit{JaccT}. Note that \textit{JaccT}  selects only appropriate synonyms to increase the similarity, but \textit{Full} utilizes all the applicable rules and most rules in \textit{Full} is useful, since it is not common that synonyms contain ambiguous meanings. In addition, note hat \textit{Full} is much more efficient than \textit{JaccT}, which will be demonstrated later.


\noindent$\bullet$ \textit{SE} significantly outperforms \textit{JaccT} in each dataset. For example, on SPROT dataset, the F-measures of \textit{SE} and \textit{JaccT} are $0.82$ and $0.52$, respectively. The main reason is that: an abbreviation may have various full expressions and the join records may contain the combination of multiple expressions. Therefore \textit{SE} can apply multiple rules, while \textit{JaccT} can apply only one. Note that such situation is not rare in the real world, as one fragment of a string likely involves multiple synonym rules. We illustrate one example on each of three datasets in Figure \ref{fig:quality} to compare the performance of four similarity measures. For example, see CONF data in Figure \ref{fig:quality}, ``\textsf{VLDB}'' has two different full expressions, i.e., ``\textsf{International Conference on Very Large Databases}'' ($r_1$) and ``\textsf{Proceedings of the VLDB Endowment}''($r_2$), and $s1$ contains these two expressions.  \textit{SE} applies both $r_1$ and $r_2$ to $s_2$ to obtain a high similarity score, i.e., 0.93, while \textit{JaccT} can only apply $r_2$ to $s_2$ and the similarity is only $0.57$. Assume that the join threshold is 0.8, then  \textit{JaccT} can not find the correct answer while \textit{SE} can.

\subsection{Efficiency and scalability of exact-join algorithms}

The second set of experiments is to test the efficiency and scalability of
various exact-joins algorithms. We compared our algorithms, the \textbf{baseline}
and \textbf{TIndex}.

Metrics.

 We took the following measures: (i) the running time (including filtering time, verification time and the time for building QP-tree for a query), and (ii) the size of candidates.

Running Time and Scalability.
Figure~\ref{fig:search_scalability_datasize} shows the running time of the four search algorithms, where the threshold is $0.9$. As shown, the running times of Search-baseline(F) and Search-baseline(S) have an exponential growth, whereas QP-search(F) and QP-search(S) scale better (i.e., linear). The reason is that QP-search generates less candidates for the final verification.

To study the scalability of algorithms with the various threshold values, we plotted Figure \ref{fig:search_scalability_threshold}. As shown, the running time of the algorithms decreases with the growth of the threshold values. In addition,  QP-search scales well with various thresholds. In contrast, when the threshold value is small, the running time of Search-baseline increases significantly. For example, in Figure \ref{fig:search_scalability_threshold}(b), when threshold=0.9, the running time of Search-baseline is about 7 times more than that of QP-search. In addition, when the threshold=0.5, the performance of QP-search is at least 15 times better than Search-baseline.

We then study the time cost of Search-baseline and QP-search with different number of synonyms. Recall that the similarity search proceeds to  generate signatures (or QP-index) for a given query, and then to filter the candidates by checking the overlaps of the signatures, and finally to verify the similarity. Therefore, we reported the individual execution time of signature-generation, filtering and verification in Figure  \ref{fig:search_synonyms_conf}. As seen from this figure, we observe that QP-Search(S) significantly outperforms Search-baseline(S) by one order of magnitude. The main reasons are as follows:



(i)  Search-baseline(S) needs to union all the signatures computed from each possible expanded set, while QP-Search(S) directly utilizes the intermediate signatures. Therefore, the time of signature-generation of QP-Search(S) is less than that of Search-baseline(S).

(ii) For the filtering phase, QP-Search(S) utilizes the SI-Index and QP-index to achieve stronger filtering power than Search-baseline(S). Thus, the filtering time of QP-Search(S) is less than Search-baseline(S). In addition, due to the powerful filtering, QP-Search(S) generates less candidates than that of Search-baseline(S). For example, in Figure  \ref{fig:search_synonyms_conf}, we reported the number of candidates for each query. As shown, the number of candidates of Search-baseline(S) is about one order of magnitude more than that of QP-search(S).

(iii)  QP-Search(S) spends less time on verifying the candidates than Search-baseline(S). Therefore, the verification time of QP-Search(S) is less than that of Search-baseline(S).

Therefore, QP-Search(S) outperforms Search-baseline(S) in each of the three phases.  Experiments in USPS and SPROT datasets have the similar trend (as shown in Figure \ref{fig:search_synonyms_usps} and Figure \ref{fig:search_synonyms_sprot}, respectively).

\subsection{Efficiency and scalability of approximate-join algorithms}

The third set of experiments is to test the efficiency and scalability of
approximate-join algorithms. We compared the algorithms \textbf{baseline}
and \textbf{SI-Join}.  We implemented all algorithms using both
prefix and LSH filters. Therefore, with respect to the algorithms using
LSH scheme, we append ``-LSH'' to the name, e.g., JaccT-LSH denotes the
JaccT algorithm using LSH scheme. Note that, we use the false negative
ratio $\delta \leq 5\%$, i.e., the accuracy is $1-\delta \geq 95\%$. The
parameters $k$ and $l$ should satisfy: $\delta \geq
(1-\theta^k)^l$~\cite{journals/tods/XiaoWLYW11}. For example, if
threshold $\theta=0.8$, $k=3$, then $l$ should be at least $5$ to
guarantee $> 95\%$ accuracy.

Metrics. We took the following measures: (i) the size of signatures, (ii) the filtering ratio of the algorithms, which is typically defined as the number~of~pruned~string~pairs divided by the total number of string pairs; and (iii) the running time (including filtering time and verification time).

Number of Signatures.


We first performed experiments to report the number of signatures of a
query string. It has a major impact on the query time, as both \textit{JaccT} and
our join algorithms need to frequently check the overlaps of string
signatures. The results are reported in Figure~\ref{fig:signature-size}.
For both prefix and LSH schemes, the number of signatures of our
expansion-based algorithms is smaller than that of \textit{JaccT}. The reason is
that based on a transformation framework, \textit{JaccT} is more likely to
include new tokens into signatures than ours. In addition, we observe
that the size of signatures in the LSH scheme is greater than that in
prefix scheme, and the gap increases when the threshold decreases. As we
will see shortly, this results in substantial overhead of filtering time
for the LSH scheme.


\textbf{Filtering Power.}

We investigated the filtering power of different algorithms in
Figures~\ref{fig:filter_ratio_prefix} and \ref{fig:filter_ratio_LSH} using
prefix and LSH schemes, respectively. The experiments were performed on
USPS and SPROT datasets for self-join. For prefix-based algorithms,
Join-baseline is slightly better than JaccT, while SI-Join has a substantial
lead over JaccT and Join-baseline. These results are mainly because of the
additional filtering power in SI-Join brought by length filtering and
signature filtering. Next, for LSH-based algorithms, Figure
\ref{fig:filter_ratio_LSH} demonstrates the similar trend. That is,
SI-Join is the winner in all settings by varying the parameters $k$ and
$l$. Compared to the prefix scheme, LSH has a slightly higher filtering
ratio when parameters are set optimally (e.g. 99.2\% v.s. 98.9\% in USPS
data, with threshold 0.9). On the other hand, note that LSH may filter
away correct answers, resulting in false negatives, as can be seen from
the false negative percentages shown in
Figure~\ref{fig:filter_ratio_LSH}.

\textbf{Effects of different filters.}

We analyzed the effects of different filters, i.e., signature-filters, length-filters and  signature-plus-length. As shown in Figure \ref{fig:filter_methods},  signature-plus-length filters obtain the strongest filtering power on all the datasets, which explains why our algorithms use these two kinds of filters together. There is no absolute winner between signature filters and length filters. In particular, signature filters have stronger filtering power on the USPS dataset (Figure \ref{fig:filter_methods}(a)), while the opposite situation occurs in the SPROT dataset (Figure \ref{fig:filter_methods}(c)), i.e., length filters beat signature filters. On the CONF dataset (Figure \ref{fig:filter_methods}(b)), when the threshold value is small, signature filters win length filters. However, when the threshold value is high, length filters beat signature filters. Therefore, the individual effects of signature filters and length filters depend on the data sets and the thresholds, and the combined structure of both (i.e. SI-tree) achieves the best results.


\textbf{Running Time and Scalability.}


Figure~\ref{fig:scalability} and Figure \ref{fig:scalability_LSH} show the running time of five join algorithms based on prefix and LSH schemes, respectively, where the join threshold is $0.8$. The x-axis represents the join data size. As shown, the running times of both JaccT and Join-baseline (i.e., Join-baseline(F), Join-baseline(S)) have an exponential growth, whereas SI-Join (i.e., SI-Join(F), SI-Join(S))  scales better (i.e., linear) than Join-baseline and JaccT. The reason is that SI-Join methods have more efficient filtering strategies and generate smaller size of candidates.  In addition, in order to study the scalability of algorithms with the increase of the number of rules, we plot Figure \ref{fig:synonyms}(b), where  SI-Join(F) (i.e., SI-Join with the full-expansion) is a clear winner in algorithms using rules: it is insensitive to the number of rules and thus able
to outperform other methods when one string involves more than 10 rules.


\subsection{Other similarity measures}

The fourth set of experiments focuses on the extensions about other similarity measures, which are discussed in Section 7.

 Experiments were conducted to empirically compare the performance of four similarity measures, i.e.,\textit{ Overlap}, \textit{Dice} and \textit{Cosine} (as mentioned in Section \ref{subsec:othersimilarity}) and \textit{Jaccard}. We compared four algorithms: QP(S)-O, QP(S)-D, QP(S)-C and QP(S)-J based on QP-search, where QP(S)-O uses \textit{Overlap} measure, QP(S)-D applies \textit{Dice} measure, QP(S)-C utilizes \textit{Cosine} measure while QP(S)-J employs \textit{Jaccard} measure. In Figure \ref{fig:different_measures}, we presented the running time of these four algorithms in three different data sets with threshold $\theta=0.9$ (for \textit{Dice}, \textit{Cosine} and \textit{Jaccard}) and $\theta=5$ (for \textit{Overlap}). As shown, QP(S)-J performs the best, since the average number of signatures of QP(S)-J is the smallest. In contrast, QP(S)-O, having a large size of signatures,  takes the longest to output its answers.

 An interesting finding is that, the precision of \textit{Full} increases when the number of noisy synonyms grows from 20 to 80, but drops to zero suddenly when the number reaches 100. It can be explained that with more noisy synonyms, less string pairs returned  from \textit{Full} have higher similarity than $0.8$. When the number of noisy synonyms goes up to 100, none of the string pairs have similarity higher than 0.8. Therefore, at this point, the precision of \textit{Full} is zero.


\subsection{Update of taxonomy trees}

The final set of experiments focuses on the update of taxonomy tress, showing that our algorithms can avoid the complete recomputation to find new results with the update of taxonomy trees.

\smallskip

\noindent \textbf{Summary} ~~ Finally, we summarize the main findings.

(1) Considering four similarity measures, we observe that \textit{Jaccard} is inadequate due to its complete neglect of synonym rules. \textit{JaccT} is not efficient because it enumerates all transformed strings and entails large query overhead. \textit{Full-expansion} is extremely efficient, but its F-measure is not as good as \textit{Selective-expansion}, which makes a good balance between effectiveness and efficiency.

(2)	With respect to the similarity searches and joins,  QP-search and SI-join are the winners in the settings, respectively over the previous algorithms (i.e., \textit{JaccT}) and baseline methods. Note that we achieve a speedup of up to 50$\sim$300x on three data sets over the state-of-the-art approach.

(3) Finally, 2DHS synopsis offers very good results, and is extremely efficient to estimate the filtering power of different filters, ( $<$ 1 second ,  accounting for only 1\% of the total running time),  which strongly motivates its application in practice.

